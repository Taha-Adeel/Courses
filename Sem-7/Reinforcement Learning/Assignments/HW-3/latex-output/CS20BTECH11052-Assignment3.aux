\relax 
\@writefile{toc}{\contentsline {section}{\numberline {Problem 1:\!\!\!\!}Model Free Methods}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)\!\!\!}Evaluate $V(s)$ using first visit Monte-Carlo method for all states $s$ of the MDP\spacefactor \@m {}. (2 points)}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)\!\!\!}Which states are likely to have different value estimates if evaluated using every visit MC as compared to first visit MC\spacefactor \@m {}? Why\spacefactor \@m {}? (2 points)}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)\!\!\!}Fill in the blank cells of the table below with the Q-values that result from applying the Q-learning update for the 4 transitions specified by the episode below. You may leave Q-values that are unaffected by the current update blank. Use learning rate $\alpha = 0.7$. Assume all $Q$-values are initialized to -10. (2 points)}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)\!\!\!}After running the Q-learning algorithm using the four transitions given above, construct a greedy policy using the current values of the Q-table in states $C$, $E$, and $F$. (1 points)}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)\!\!\!}For the Q-Learning algorithm to converge to the optimal Q function, a necessary condition is that the learning rate, $\alpha _t$, which is the learning rate at the $t-$th time step would need to satisfy the Robinns-Monroe condtion. In here, the time step $t$ refers to the $t-$th time we are updating the value of the Q value of the state-action pair $(s, a)$. Would the following values for learning rate $\alpha _t$ obey Robbins Monroe conditions? (3 points)}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(f)\!\!\!}A RL agent collects experiences of the form $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ to update $Q$ values. At each time step, to choose an action, the agent follows a fixed policy $\pi $ with probablity 0.5 or chooses an action in uniform random fashion. Assume the updates are applied infinitely often, state-action pairs are visited infintely often, the discount factor $\gamma < 1$ and the learning rate scheduling is appropriate.}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {Problem 2:\!\!\!\!}Game of Tic-Tac-Toe}{4}{}\protected@file@percent }
\gdef \@abspage@last{4}

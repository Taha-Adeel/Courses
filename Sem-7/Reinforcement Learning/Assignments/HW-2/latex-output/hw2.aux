\relax 
\@writefile{toc}{\contentsline {section}{\numberline {Problem 1:\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Value Iteration}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Prove that the Bellman optimality operator is a contraction under the max-norm. (5 points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Prove that the iterative policy evalution algorithm converges geometrically. (3 points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Let M be an infinite horizon MDP and $V^*$ be its optimal value function. Suppose if the value iteration algorithm is terminated after $k + 1$ iterations as $\|V_{k+1} - V_k\|_\infty < \epsilon $ for some chosen $\epsilon > 0$, how far is the estimate $V_{k+1}$ from the optimal value function $V^*$? Provide details of your derivation. (5 points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {Problem 2:\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Programming Value Iteration}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Implement value iteration and policy iteration algorithm. Modularize the implementation to contain separate functions for policy evalution, value iteration and policy improvement. Test the implementation on the 4 x 4 Frozen Lake environment available in Gymnasium (formerly known as Open AI Gym).The stochastic version of the environment is default which can be changed by modifying the is-slippery flag. (8 points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Document your findings with number of iterations needed for both algorithms to converge (or nearly converge) to optimal policy on the Frozen Lake envrionment. Further, provide a snapshot of the optimal policy obtained via the two algorithms. (4 points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Are there any stochastic optimal policies? If so, does any of the algorithms find any stochastic optimal policy? If not, why not? (3 points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Noisy Environment}{1}\protected@file@percent }

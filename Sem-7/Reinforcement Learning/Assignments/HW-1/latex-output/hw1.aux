\relax 
\@writefile{toc}{\contentsline {section}{\numberline {Problem 1:\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Markov Reward process}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Identify the states, transition probablities and terminal states (if any) of the MRP. (3 Points)}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Markov Chain}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Construct a suitable reward function, discount factor and use the Bellman equation for MRP to find the 'average' number of tosses required for the pattern '1234' to appear. (7 Points)}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {Problem 2:\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Markov Decision Process}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Let M be an infinite horizon MDP given by $< \mathcal  {S}, \mathcal  {A}, \mathcal  {P}, \mathcal  {R}, \gamma >$ with $|\mathcal  {S}| < \infty $ and $|\mathcal  {A}| < \infty $ and $\gamma \in [0, 1)$. Suppose that the reward function $\mathcal  {R}^a_{ss'}$ for any successor states $s, s' \in \mathcal  {S}$ and action $a \in \mathcal  {A}$ is non-negative and bounded, what is the lower and upper bound on the discounted sum of rewards? (3 Points)}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Let $\hat  {M} = < \mathcal  {S}, \mathcal  {A}, \mathcal  {P}, \hat  {\mathcal  {R}}, \gamma >$ be another infinite horizon MDP with a modified reward function $\hat  {\mathcal  {R}}$ such that $$\mathcal  {R}^a_{ss'} - \hat  {\mathcal  {R}}^a_{ss'} = \epsilon ,$$ where $\epsilon $ is a constant independent of $s \in \mathcal  {S}$ or $a \in \mathcal  {A}$. Given a policy $\pi $, let $V^\pi $ and $ \hat  {V}^\pi $ be value functions of policy $\pi $ for MDPs $M$ and $\hat  {M}$ respectively. Derive an expression that relates $V^\pi (s)$ to $\hat  {V}^\pi (s)$ for any state $s \in \mathcal  {S}$ of the MDP. (3 Points)}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Does $M$ and $\hat  {M}$ have the same optimal policy ? Explain. (3 Points)}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}From sub-question (b) can one argue that the assumption that the MDP $M$ in sub-question (a) has non-negative and bounded reward is without loss in generality ? What if the MDP $M$ is allowed to have negative but bounded rewards ? (3 Points)}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}State and prove an analogous result for the sub-question (b) for the case when $M$ and $\hat  {M}$ are finite horizon MDPs with horizon length $H < \infty $. (4 Points)}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(f)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Now, consider an indefinite MDP or a stochastic shortest path MDP where the horizon length $H$ can vary. A subset of the state space $S_{term} \subset \mathcal  {S}$ is considered terminal if a trajectory of the form $s_0, a_0, r_1, s_1, a_1, r_2, \cdots  ,$ keeps rolling out until a terminal state $S_H \in \mathcal  {S}$ term is visited. In general, the length of the episode $H$ is a random variable. Does the analogous result of sub-question (b) hold when $M$ and $\hat  {M}$ are indefinite MDPs ? Explain. (4 Points)}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(g)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}For this sub-question, let $\hat  {M} = < \mathcal  {S}, \mathcal  {A}, \mathcal  {P}, \hat  {\mathcal  {R}}, \gamma >$ be a infinite horizon MDP with a modified reward function $\hat  {\mathcal  {R}}$ such that $$ \mathcal  {R}^a_{ss'} - \hat  {\mathcal  {R}}^a_{ss'} \leq \epsilon $$ where $\epsilon $ is a constant independent of $s$ and $a$. Derive an expression that relates the optimal value functions $V_* (s)$ to $\hat  {V}_* (s)$. Would $M$ and $\hat  {M}$ have the same optimal policy ? Explain. (6 Points)}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {(h)\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}}Now consider the MDP $M$ of sub-question (a). Does scaling the discount factor by a constant $\kappa \in (0, 1)$ alter the optimal policy ? Explain. (4 Points)}{7}\protected@file@percent }
